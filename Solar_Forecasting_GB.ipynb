{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea52c46a-7fde-44b6-82f6-dd090112d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab30cc41-b63d-4054-9cf3-1c40d1dcc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Untitled spreadsheet - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b296e02-bcdc-4075-9b85-b53ee3ac06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size = int(len(df) * 0.8)  # 80% data for training, 20% for testing\n",
    "#train_data, test_data = df['PolyPwr'][:train_size], df['PolyPwr'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c60cf597-e53a-4822-b14f-56962a751d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.dtypes)\n",
    "#print(df['PolyPwr'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59fffd3b-4434-4ac3-93a5-947608918ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ARIMA(train_data, order=(1, 1, 5))\n",
    "#results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98526b80-e94a-41ed-8b94-6a00f7d65009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_values = results.forecast(steps=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "011dd506-f619-497a-8151-5ba48fedda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_value = results.forecast(steps=1)  # The forecast for the next time point\n",
    "\n",
    "#print(\"Forecasted value:\", forecast_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c20c7037-c02f-463d-8226-6e4578e74566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 5.557085643886112\n",
      "Forecast for PolyPwr: 10.592899507462686\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1000]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6422c911-2f82-4551-a93e-d8b5976b3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 6.398862438123686\n",
      "Forecast for PolyPwr: 12.988465720145852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1100]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9390bb9b-6fef-40f2-ad1c-5272c6bf1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 7.174903427478732\n",
      "Forecast for PolyPwr: 14.789816320560364\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1200]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3df954f-cc68-4756-8cec-35aecb7d698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 7.161785606180949\n",
      "Forecast for PolyPwr: 15.354253299131805\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1300]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a16387-578d-4700-8ef6-9dfd549dd63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 7.250610576498969\n",
      "Forecast for PolyPwr: 14.66241980878763\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1400]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2cf163-1eae-4c3d-8a03-f9d5dd5e6dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 6.784903639652876\n",
      "Forecast for PolyPwr: 13.00853669511249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load and Preprocess the Data\n",
    "# Assuming you have the dataset in a pandas DataFrame with columns X, PolyPwr, and Y\n",
    "# Replace 'your_dataset.csv' with the actual file name if loading from a CSV\n",
    "df = pd.read_csv('Pasion et al dataset.csv')\n",
    "data = df[df['Location'] == 'Hill Weber']\n",
    "data = df[df['Time'] == 1500]\n",
    "\n",
    "# Extract the target column 'PolyPwr'\n",
    "data = data[['Date', 'PolyPwr']]  # Select only the relevant columns for the time series\n",
    "data.rename(columns={'Date': 'ds', 'PolyPwr': 'y'}, inplace=True)\n",
    "\n",
    "# Convert the 'ds' column to datetime type\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Create features for year, month, day, hour, etc.\n",
    "data['year'] = data['ds'].dt.year\n",
    "data['month'] = data['ds'].dt.month\n",
    "data['day'] = data['ds'].dt.day\n",
    "data['hour'] = data['ds'].dt.hour\n",
    "\n",
    "# Step 3: Normalize the Data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['ds', 'y']))  # Exclude 'ds' and 'y' columns\n",
    "\n",
    "# Step 4: Prepare the Data for GBR\n",
    "X = data_scaled\n",
    "y = data['y']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Model Creation and Training\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "predictions = gbr_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Step 7: Forecasting\n",
    "# Prepare exogenous input data for forecasting\n",
    "last_observed = X_test[-1].reshape(1, -1)  # Use the last observed feature values for forecasting\n",
    "forecast_value = gbr_model.predict(last_observed)\n",
    "\n",
    "# Print the forecasted value for 'PolyPwr'\n",
    "print(\"Forecast for PolyPwr:\", forecast_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43bed2-3a05-47c2-b781-19f96d6c28f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
